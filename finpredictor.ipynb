{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Imports and Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_components.data_reader import FinancialTimeSeriesDataset, verify_financial_dataloader\n",
    "from model_components.transformers import TimeSeriesTransformer\n",
    "from model_components.fin_trainer import TimeSeriesForecastingTrainer\n",
    "from model_components.utils import (\n",
    "    create_optimizer,\n",
    "    create_scheduler,\n",
    "    plot_lr_schedule\n",
    ")\n",
    "  \n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import gc\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import shutil\n",
    "import wandb\n",
    "import yaml\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "Name                      : \"Nayesha-Gopal\"\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "data:                    # Currently setup for Colab assuming out setup\n",
    "  root                 : \"/local/hw4_data/hw4p1_data\"  # TODO: Set the root path of your data\n",
    "  train_partition      : \"train\"  # train\n",
    "  val_partition        : \"val\"    # val\n",
    "  test_partition       : \"test\"   # test\n",
    "  subset               : 1.0      # Load a subset of the data (for debugging, testing, etc\n",
    "  batch_size           : 256      #\n",
    "  NUM_WORKERS          : 2        # Set to 0 for CPU\n",
    "  forecast_horizon     : 5\n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "model: # Decoder-Only Language Model (HW4P1)\n",
    "  d_model                   : 256\n",
    "  d_ff                      : 1024\n",
    "  d_freq                    : 3    #how many dimensions to split time into- must be odd because it adds 1?\n",
    "  num_layers                : 2\n",
    "  num_heads                 : 2\n",
    "  dropout                   : 0.0\n",
    "\n",
    "###### Common Training Parameters ------------------------------------------------\n",
    "training:\n",
    "  use_wandb                   : True   # Toggle wandb logging\n",
    "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
    "  resume                      : False  # Resume an existing run (run_id != 'none')\n",
    "  epochs                      : 20\n",
    "  gradient_accumulation_steps : 1\n",
    "  wandb_project               : \"IDL_final project PegasusNG\" # wandb project to log to\n",
    "\n",
    "###### Loss ----------------------------------------------------------------------\n",
    "loss: # Just good ol' CrossEntropy\n",
    "  label_smoothing: 0.0\n",
    "\n",
    "###### Optimizer -----------------------------------------------------------------\n",
    "optimizer:\n",
    "  name: \"adamw\" # Options: sgd, adam, adamw\n",
    "  lr: 5.0e-4   # Base learning rate\n",
    "\n",
    "  # Common parameters\n",
    "  weight_decay: 0.0001\n",
    "\n",
    "  # Parameter groups\n",
    "  param_groups:\n",
    "    - name: self_attn\n",
    "      patterns: []  # Will match all parameters containing keywords set their learning rate to 0.0001\n",
    "      lr: 0.0001    # LR for self_attn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "    - name: ffn\n",
    "      patterns: [] # Will match all parameters containing \"ffn\" and set their learning rate to 0.0001\n",
    "      lr: 0.0001   # LR for ffn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "\n",
    "  # Layer-wise learning rates\n",
    "  layer_decay:\n",
    "    enabled: False\n",
    "    decay_rate: 0.75\n",
    "\n",
    "  # SGD specific parameters\n",
    "  sgd:\n",
    "    momentum: 0.9\n",
    "    nesterov: True\n",
    "    dampening: 0\n",
    "\n",
    "  # Adam specific parameters\n",
    "  adam:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "  # AdamW specific parameters\n",
    "  adamw:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "###### Scheduler -----------------------------------------------------------------\n",
    "scheduler:\n",
    "  name: \"cosine\"  # Options: reduce_lr, cosine, cosine_warm\n",
    "\n",
    "  # ReduceLROnPlateau specific parameters\n",
    "  reduce_lr:\n",
    "    mode: \"min\"  # Options: min, max\n",
    "    factor: 0.1  # Factor to reduce learning rate by\n",
    "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
    "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
    "    threshold_mode: \"rel\"  # Options: rel, abs\n",
    "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
    "    min_lr: 0.0000001  # Minimum learning rate\n",
    "    eps: 1.0e-8  # Minimal decay applied to lr\n",
    "\n",
    "  # CosineAnnealingLR specific parameters\n",
    "  cosine:\n",
    "    T_max: 15  # Maximum number of iterations\n",
    "    eta_min: 1.0e-8  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # CosineAnnealingWarmRestarts specific parameters\n",
    "  cosine_warm:\n",
    "    T_0: 4  # Number of iterations for the first restart\n",
    "    T_mult: 4  # Factor increasing T_i after each restart\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # Warmup parameters (can be used with any scheduler)\n",
    "  warmup:\n",
    "    enabled: True\n",
    "    type: \"exponential\"  # Options: linear, exponential\n",
    "    epochs: 5\n",
    "    start_factor: 0.1\n",
    "    end_factor: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can add a val ratio element to this as well\n",
    "train_dataset = FinancialTimeSeriesDataset(\n",
    "    tickers=['^SPX'],\n",
    "    start_date='2015-01-01',\n",
    "    end_date='2024-12-31',\n",
    "    window_size=30,\n",
    "    forecast_horizon=config['data']['forecast_horizon'],\n",
    "    target='Close',\n",
    "    normalize='zscore',\n",
    "    split='train',\n",
    "    val_ratio=0.25\n",
    ")\n",
    "\n",
    "val_dataset = FinancialTimeSeriesDataset(\n",
    "    tickers=['^SPX'],\n",
    "    start_date='2015-01-01',\n",
    "    end_date='2024-12-31',\n",
    "    window_size=30,\n",
    "    forecast_horizon=config['data']['forecast_horizon'],\n",
    "    target='Close',\n",
    "    normalize='zscore',\n",
    "    split='val',\n",
    "    val_ratio=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADERS\n",
    "train_loader    = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = True,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    ")\n",
    "\n",
    "val_loader      = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify dataloader\n",
    "verify_financial_dataloader(train_loader)\n",
    "verify_financial_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config['model']\n",
    "# update this to add model input and output features\n",
    "print(train_dataset.input_features, val_dataset.output_features)\n",
    "model_config.update({\n",
    "    'input_features': train_dataset.input_features,\n",
    "    'output_features': val_dataset.output_features,              #hardcoded, TODO NEED TO FIX\n",
    "    'forecast_horizon':  config['data']['forecast_horizon']\n",
    "})\n",
    "model = TimeSeriesTransformer(**model_config)\n",
    "\n",
    "# CONFIRM EVERYTHING IS BEING LOADED CORRECTLY\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "\n",
    "    print(\"First input sample (shape: {}):\".format(inputs[0].shape))\n",
    "    print(inputs[0])  # shape: [T, F]\n",
    "\n",
    "    print(\"Corresponding target (shape: {}):\".format(targets[0].shape))\n",
    "    print(targets[0])  # shape: [forecast_steps]\n",
    "\n",
    "    print(\"Inputs:\", inputs.shape)\n",
    "    print(\"Targets:\", targets.shape)\n",
    "    break\n",
    "\n",
    "outputs = model(inputs)  # or model(inputs, something_else) if needed\n",
    "print(outputs.shape)\n",
    "\n",
    "model_stats = summary(model, input_data=inputs.to(\"cpu\"), device=\"cpu\")\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Setup WandB and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"b78dba2c54228a3e3e32bd91a1a1efca38d69bb9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TimeSeriesForecastingTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    run_name=\"test-1-fin\",\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.optimizer = create_optimizer(\n",
    "    model=model,\n",
    "    opt_config=config['optimizer']\n",
    ")\n",
    "\n",
    "test_scheduler = create_scheduler(\n",
    "    optimizer=trainer.optimizer,\n",
    "    scheduler_config=config['scheduler'],\n",
    "    train_loader=train_loader,\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
    ")\n",
    "\n",
    "plot_lr_schedule(\n",
    "    scheduler=test_scheduler,\n",
    "    num_epochs=config['training']['epochs'],\n",
    "    train_loader=train_loader,\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
    ")\n",
    "\n",
    "trainer.scheduler = create_scheduler(\n",
    "    optimizer=trainer.optimizer,\n",
    "    scheduler_config=config['scheduler'],\n",
    "    train_loader=train_loader,\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: TRAIN!!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=config['training']['epochs'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
